{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_Tzn4BWBGvR"
   },
   "source": [
    "# Building a Neural Network from Scratch\n",
    "\n",
    "https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3zzD8_kBGvW"
   },
   "source": [
    "### Abstract Base Class : Layer\n",
    "The abstract class Layer, which all other layers will inherit from, handles simple properties which are an input, an output, and both a forward and backward methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hh9ejRlYBGvX"
   },
   "outputs": [],
   "source": [
    "# Base class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTPVmMjCBGvY"
   },
   "source": [
    "In the abstract class above, backward_propagation function has an extra parameter, learning_rate, which is controlling the amount of learning/updating parameters using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaQCaN9IBGvY"
   },
   "source": [
    "### Backward Propagation\n",
    "Suppose we have a matrix containing the derivative of the error with respect to that layer’s output: $\\frac{\\partial E}{\\partial Y}$\n",
    "\n",
    "We need :\n",
    "- The derivative of the error with respect to the parameters ($\\frac{\\partial E}{\\partial W}$, $\\frac{\\partial E}{\\partial B}$)\n",
    "- The derivative of the error with respect to the input ($\\frac{\\partial E}{\\partial X}$)\n",
    "\n",
    "Let's calculate $\\frac{\\partial E}{\\partial W}$. This matrix should be the same size as $W$ itself : \n",
    "\n",
    "$i x j$ where $i$ is the number of input neurons and $j$ the number of output neurons. We need one gradient for every weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXPG6UXjBGvZ"
   },
   "source": [
    "### Coding the Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FMaxpwBHBGvZ"
   },
   "outputs": [],
   "source": [
    "#from layer import Layer\n",
    "import numpy as np\n",
    "\n",
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of edges that connects to neurons in next layer\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnmset1rBGva"
   },
   "source": [
    "### Activation Layer\n",
    "All the calculation we did until now were completely linear, may not learn well. We need to add non-linearity to the model by applying non-linear functions to the output of some layers.\n",
    "\n",
    "Now we need to redo the whole process for this new type of layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jMiJEWGTBGva"
   },
   "outputs": [],
   "source": [
    "#from layer import Layer\n",
    "\n",
    "# inherit from base class Layer\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7LPwebSBGvb"
   },
   "source": [
    "You can also write some activation functions and their derivatives in a separate file. These will be used later to create an ActivationLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n-rLr5eRBGvb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# activation function and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1962I88SBGvc"
   },
   "source": [
    "### Loss Function\n",
    "Until now, for a given layer, we supposed that ∂E/∂Y was given (by the next layer). But what happens to the last layer? How does it get ∂E/∂Y? We simply give it manually, and it depends on how we define the error.\n",
    "The error of the network, which measures how good or bad the network did for a given input data, is defined by you. \n",
    "\n",
    "There are many ways to define the error, and one of the most known is called MSE — Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "b8WftiKrBGvc"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVorbeubBGvc"
   },
   "source": [
    "### Network Class\n",
    "Almost done ! We are going to make a Network class to create neural networks very easily using the building blocks we have prepared so far.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mdiW_fBfBGvc"
   },
   "outputs": [],
   "source": [
    "# example of a function for calculating softmax for a list of numbers\n",
    "from numpy import exp\n",
    " \n",
    "# calculate the softmax of a vector\n",
    "def softmax(vector):\n",
    "    e = exp(vector)\n",
    "    return e / e.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2ju-hpdYBGvd"
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "        \n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network \n",
    "    \n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        '''\n",
    "        Fit function does the training. \n",
    "        Training data is passed 1-by-1 through the network layers during forward propagation.\n",
    "        Loss (error) is calculated for each input and back propagation is performed via partial \n",
    "        derivatives on each layer.\n",
    "        '''\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wQfn-s5BGvd"
   },
   "source": [
    "### Building Neural Networks\n",
    "Finally ! We can use our class to create a neural network with as many layers as we want ! We are going to build two neural networks : a simple XOR and a MNIST solver.\n",
    "\n",
    "\n",
    "### Solve XOR\n",
    "Starting with XOR is always important as it’s a simple way to tell if the network is learning anything at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KJfKmAHhBGvd",
    "outputId": "33d0b062-4c6b-4aac-8b6b-9ca3b1b64476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000   error=0.988294\n",
      "epoch 2/1000   error=0.392466\n",
      "epoch 3/1000   error=0.316818\n",
      "epoch 4/1000   error=0.303850\n",
      "epoch 5/1000   error=0.299938\n",
      "epoch 6/1000   error=0.298213\n",
      "epoch 7/1000   error=0.297206\n",
      "epoch 8/1000   error=0.296487\n",
      "epoch 9/1000   error=0.295903\n",
      "epoch 10/1000   error=0.295391\n",
      "epoch 11/1000   error=0.294920\n",
      "epoch 12/1000   error=0.294478\n",
      "epoch 13/1000   error=0.294058\n",
      "epoch 14/1000   error=0.293654\n",
      "epoch 15/1000   error=0.293267\n",
      "epoch 16/1000   error=0.292893\n",
      "epoch 17/1000   error=0.292532\n",
      "epoch 18/1000   error=0.292183\n",
      "epoch 19/1000   error=0.291846\n",
      "epoch 20/1000   error=0.291521\n",
      "epoch 21/1000   error=0.291206\n",
      "epoch 22/1000   error=0.290902\n",
      "epoch 23/1000   error=0.290608\n",
      "epoch 24/1000   error=0.290324\n",
      "epoch 25/1000   error=0.290049\n",
      "epoch 26/1000   error=0.289783\n",
      "epoch 27/1000   error=0.289527\n",
      "epoch 28/1000   error=0.289278\n",
      "epoch 29/1000   error=0.289039\n",
      "epoch 30/1000   error=0.288807\n",
      "epoch 31/1000   error=0.288583\n",
      "epoch 32/1000   error=0.288367\n",
      "epoch 33/1000   error=0.288158\n",
      "epoch 34/1000   error=0.287957\n",
      "epoch 35/1000   error=0.287762\n",
      "epoch 36/1000   error=0.287574\n",
      "epoch 37/1000   error=0.287392\n",
      "epoch 38/1000   error=0.287217\n",
      "epoch 39/1000   error=0.287048\n",
      "epoch 40/1000   error=0.286884\n",
      "epoch 41/1000   error=0.286727\n",
      "epoch 42/1000   error=0.286575\n",
      "epoch 43/1000   error=0.286428\n",
      "epoch 44/1000   error=0.286286\n",
      "epoch 45/1000   error=0.286149\n",
      "epoch 46/1000   error=0.286017\n",
      "epoch 47/1000   error=0.285889\n",
      "epoch 48/1000   error=0.285766\n",
      "epoch 49/1000   error=0.285647\n",
      "epoch 50/1000   error=0.285532\n",
      "epoch 51/1000   error=0.285421\n",
      "epoch 52/1000   error=0.285314\n",
      "epoch 53/1000   error=0.285210\n",
      "epoch 54/1000   error=0.285110\n",
      "epoch 55/1000   error=0.285013\n",
      "epoch 56/1000   error=0.284919\n",
      "epoch 57/1000   error=0.284828\n",
      "epoch 58/1000   error=0.284741\n",
      "epoch 59/1000   error=0.284656\n",
      "epoch 60/1000   error=0.284573\n",
      "epoch 61/1000   error=0.284493\n",
      "epoch 62/1000   error=0.284415\n",
      "epoch 63/1000   error=0.284340\n",
      "epoch 64/1000   error=0.284267\n",
      "epoch 65/1000   error=0.284196\n",
      "epoch 66/1000   error=0.284126\n",
      "epoch 67/1000   error=0.284058\n",
      "epoch 68/1000   error=0.283992\n",
      "epoch 69/1000   error=0.283928\n",
      "epoch 70/1000   error=0.283865\n",
      "epoch 71/1000   error=0.283803\n",
      "epoch 72/1000   error=0.283743\n",
      "epoch 73/1000   error=0.283683\n",
      "epoch 74/1000   error=0.283625\n",
      "epoch 75/1000   error=0.283568\n",
      "epoch 76/1000   error=0.283511\n",
      "epoch 77/1000   error=0.283455\n",
      "epoch 78/1000   error=0.283400\n",
      "epoch 79/1000   error=0.283345\n",
      "epoch 80/1000   error=0.283290\n",
      "epoch 81/1000   error=0.283236\n",
      "epoch 82/1000   error=0.283182\n",
      "epoch 83/1000   error=0.283128\n",
      "epoch 84/1000   error=0.283075\n",
      "epoch 85/1000   error=0.283021\n",
      "epoch 86/1000   error=0.282967\n",
      "epoch 87/1000   error=0.282912\n",
      "epoch 88/1000   error=0.282858\n",
      "epoch 89/1000   error=0.282802\n",
      "epoch 90/1000   error=0.282747\n",
      "epoch 91/1000   error=0.282690\n",
      "epoch 92/1000   error=0.282633\n",
      "epoch 93/1000   error=0.282575\n",
      "epoch 94/1000   error=0.282515\n",
      "epoch 95/1000   error=0.282455\n",
      "epoch 96/1000   error=0.282393\n",
      "epoch 97/1000   error=0.282330\n",
      "epoch 98/1000   error=0.282266\n",
      "epoch 99/1000   error=0.282200\n",
      "epoch 100/1000   error=0.282132\n",
      "epoch 101/1000   error=0.282062\n",
      "epoch 102/1000   error=0.281990\n",
      "epoch 103/1000   error=0.281916\n",
      "epoch 104/1000   error=0.281839\n",
      "epoch 105/1000   error=0.281761\n",
      "epoch 106/1000   error=0.281679\n",
      "epoch 107/1000   error=0.281595\n",
      "epoch 108/1000   error=0.281507\n",
      "epoch 109/1000   error=0.281417\n",
      "epoch 110/1000   error=0.281323\n",
      "epoch 111/1000   error=0.281225\n",
      "epoch 112/1000   error=0.281124\n",
      "epoch 113/1000   error=0.281019\n",
      "epoch 114/1000   error=0.280909\n",
      "epoch 115/1000   error=0.280795\n",
      "epoch 116/1000   error=0.280677\n",
      "epoch 117/1000   error=0.280553\n",
      "epoch 118/1000   error=0.280424\n",
      "epoch 119/1000   error=0.280290\n",
      "epoch 120/1000   error=0.280150\n",
      "epoch 121/1000   error=0.280004\n",
      "epoch 122/1000   error=0.279852\n",
      "epoch 123/1000   error=0.279692\n",
      "epoch 124/1000   error=0.279526\n",
      "epoch 125/1000   error=0.279352\n",
      "epoch 126/1000   error=0.279171\n",
      "epoch 127/1000   error=0.278982\n",
      "epoch 128/1000   error=0.278784\n",
      "epoch 129/1000   error=0.278576\n",
      "epoch 130/1000   error=0.278360\n",
      "epoch 131/1000   error=0.278134\n",
      "epoch 132/1000   error=0.277897\n",
      "epoch 133/1000   error=0.277650\n",
      "epoch 134/1000   error=0.277392\n",
      "epoch 135/1000   error=0.277122\n",
      "epoch 136/1000   error=0.276839\n",
      "epoch 137/1000   error=0.276544\n",
      "epoch 138/1000   error=0.276236\n",
      "epoch 139/1000   error=0.275914\n",
      "epoch 140/1000   error=0.275577\n",
      "epoch 141/1000   error=0.275225\n",
      "epoch 142/1000   error=0.274858\n",
      "epoch 143/1000   error=0.274475\n",
      "epoch 144/1000   error=0.274076\n",
      "epoch 145/1000   error=0.273659\n",
      "epoch 146/1000   error=0.273224\n",
      "epoch 147/1000   error=0.272771\n",
      "epoch 148/1000   error=0.272300\n",
      "epoch 149/1000   error=0.271809\n",
      "epoch 150/1000   error=0.271298\n",
      "epoch 151/1000   error=0.270767\n",
      "epoch 152/1000   error=0.270216\n",
      "epoch 153/1000   error=0.269644\n",
      "epoch 154/1000   error=0.269050\n",
      "epoch 155/1000   error=0.268435\n",
      "epoch 156/1000   error=0.267799\n",
      "epoch 157/1000   error=0.267141\n",
      "epoch 158/1000   error=0.266461\n",
      "epoch 159/1000   error=0.265760\n",
      "epoch 160/1000   error=0.265038\n",
      "epoch 161/1000   error=0.264294\n",
      "epoch 162/1000   error=0.263530\n",
      "epoch 163/1000   error=0.262746\n",
      "epoch 164/1000   error=0.261943\n",
      "epoch 165/1000   error=0.261121\n",
      "epoch 166/1000   error=0.260282\n",
      "epoch 167/1000   error=0.259426\n",
      "epoch 168/1000   error=0.258555\n",
      "epoch 169/1000   error=0.257670\n",
      "epoch 170/1000   error=0.256773\n",
      "epoch 171/1000   error=0.255864\n",
      "epoch 172/1000   error=0.254946\n",
      "epoch 173/1000   error=0.254021\n",
      "epoch 174/1000   error=0.253089\n",
      "epoch 175/1000   error=0.252153\n",
      "epoch 176/1000   error=0.251214\n",
      "epoch 177/1000   error=0.250275\n",
      "epoch 178/1000   error=0.249338\n",
      "epoch 179/1000   error=0.248403\n",
      "epoch 180/1000   error=0.247473\n",
      "epoch 181/1000   error=0.246548\n",
      "epoch 182/1000   error=0.245632\n",
      "epoch 183/1000   error=0.244725\n",
      "epoch 184/1000   error=0.243828\n",
      "epoch 185/1000   error=0.242943\n",
      "epoch 186/1000   error=0.242071\n",
      "epoch 187/1000   error=0.241212\n",
      "epoch 188/1000   error=0.240367\n",
      "epoch 189/1000   error=0.239537\n",
      "epoch 190/1000   error=0.238723\n",
      "epoch 191/1000   error=0.237924\n",
      "epoch 192/1000   error=0.237142\n",
      "epoch 193/1000   error=0.236375\n",
      "epoch 194/1000   error=0.235624\n",
      "epoch 195/1000   error=0.234889\n",
      "epoch 196/1000   error=0.234169\n",
      "epoch 197/1000   error=0.233465\n",
      "epoch 198/1000   error=0.232776\n",
      "epoch 199/1000   error=0.232101\n",
      "epoch 200/1000   error=0.231440\n",
      "epoch 201/1000   error=0.230793\n",
      "epoch 202/1000   error=0.230159\n",
      "epoch 203/1000   error=0.229537\n",
      "epoch 204/1000   error=0.228926\n",
      "epoch 205/1000   error=0.228327\n",
      "epoch 206/1000   error=0.227738\n",
      "epoch 207/1000   error=0.227159\n",
      "epoch 208/1000   error=0.226589\n",
      "epoch 209/1000   error=0.226028\n",
      "epoch 210/1000   error=0.225474\n",
      "epoch 211/1000   error=0.224927\n",
      "epoch 212/1000   error=0.224387\n",
      "epoch 213/1000   error=0.223852\n",
      "epoch 214/1000   error=0.223321\n",
      "epoch 215/1000   error=0.222796\n",
      "epoch 216/1000   error=0.222273\n",
      "epoch 217/1000   error=0.221753\n",
      "epoch 218/1000   error=0.221236\n",
      "epoch 219/1000   error=0.220719\n",
      "epoch 220/1000   error=0.220203\n",
      "epoch 221/1000   error=0.219687\n",
      "epoch 222/1000   error=0.219170\n",
      "epoch 223/1000   error=0.218650\n",
      "epoch 224/1000   error=0.218128\n",
      "epoch 225/1000   error=0.217603\n",
      "epoch 226/1000   error=0.217072\n",
      "epoch 227/1000   error=0.216536\n",
      "epoch 228/1000   error=0.215994\n",
      "epoch 229/1000   error=0.215444\n",
      "epoch 230/1000   error=0.214885\n",
      "epoch 231/1000   error=0.214316\n",
      "epoch 232/1000   error=0.213736\n",
      "epoch 233/1000   error=0.213143\n",
      "epoch 234/1000   error=0.212537\n",
      "epoch 235/1000   error=0.211915\n",
      "epoch 236/1000   error=0.211276\n",
      "epoch 237/1000   error=0.210618\n",
      "epoch 238/1000   error=0.209940\n",
      "epoch 239/1000   error=0.209239\n",
      "epoch 240/1000   error=0.208514\n",
      "epoch 241/1000   error=0.207761\n",
      "epoch 242/1000   error=0.206980\n",
      "epoch 243/1000   error=0.206166\n",
      "epoch 244/1000   error=0.205317\n",
      "epoch 245/1000   error=0.204430\n",
      "epoch 246/1000   error=0.203502\n",
      "epoch 247/1000   error=0.202528\n",
      "epoch 248/1000   error=0.201506\n",
      "epoch 249/1000   error=0.200430\n",
      "epoch 250/1000   error=0.199296\n",
      "epoch 251/1000   error=0.198099\n",
      "epoch 252/1000   error=0.196834\n",
      "epoch 253/1000   error=0.195493\n",
      "epoch 254/1000   error=0.194071\n",
      "epoch 255/1000   error=0.192561\n",
      "epoch 256/1000   error=0.190955\n",
      "epoch 257/1000   error=0.189244\n",
      "epoch 258/1000   error=0.187420\n",
      "epoch 259/1000   error=0.185472\n",
      "epoch 260/1000   error=0.183391\n",
      "epoch 261/1000   error=0.181165\n",
      "epoch 262/1000   error=0.178781\n",
      "epoch 263/1000   error=0.176229\n",
      "epoch 264/1000   error=0.173495\n",
      "epoch 265/1000   error=0.170565\n",
      "epoch 266/1000   error=0.167428\n",
      "epoch 267/1000   error=0.164071\n",
      "epoch 268/1000   error=0.160482\n",
      "epoch 269/1000   error=0.156651\n",
      "epoch 270/1000   error=0.152571\n",
      "epoch 271/1000   error=0.148238\n",
      "epoch 272/1000   error=0.143653\n",
      "epoch 273/1000   error=0.138821\n",
      "epoch 274/1000   error=0.133754\n",
      "epoch 275/1000   error=0.128471\n",
      "epoch 276/1000   error=0.122999\n",
      "epoch 277/1000   error=0.117371\n",
      "epoch 278/1000   error=0.111630\n",
      "epoch 279/1000   error=0.105821\n",
      "epoch 280/1000   error=0.099996\n",
      "epoch 281/1000   error=0.094207\n",
      "epoch 282/1000   error=0.088506\n",
      "epoch 283/1000   error=0.082943\n",
      "epoch 284/1000   error=0.077561\n",
      "epoch 285/1000   error=0.072397\n",
      "epoch 286/1000   error=0.067482\n",
      "epoch 287/1000   error=0.062836\n",
      "epoch 288/1000   error=0.058472\n",
      "epoch 289/1000   error=0.054396\n",
      "epoch 290/1000   error=0.050607\n",
      "epoch 291/1000   error=0.047099\n",
      "epoch 292/1000   error=0.043861\n",
      "epoch 293/1000   error=0.040881\n",
      "epoch 294/1000   error=0.038144\n",
      "epoch 295/1000   error=0.035632\n",
      "epoch 296/1000   error=0.033330\n",
      "epoch 297/1000   error=0.031221\n",
      "epoch 298/1000   error=0.029289\n",
      "epoch 299/1000   error=0.027519\n",
      "epoch 300/1000   error=0.025895\n",
      "epoch 301/1000   error=0.024405\n",
      "epoch 302/1000   error=0.023037\n",
      "epoch 303/1000   error=0.021779\n",
      "epoch 304/1000   error=0.020621\n",
      "epoch 305/1000   error=0.019553\n",
      "epoch 306/1000   error=0.018568\n",
      "epoch 307/1000   error=0.017657\n",
      "epoch 308/1000   error=0.016813\n",
      "epoch 309/1000   error=0.016031\n",
      "epoch 310/1000   error=0.015305\n",
      "epoch 311/1000   error=0.014630\n",
      "epoch 312/1000   error=0.014002\n",
      "epoch 313/1000   error=0.013416\n",
      "epoch 314/1000   error=0.012868\n",
      "epoch 315/1000   error=0.012356\n",
      "epoch 316/1000   error=0.011877\n",
      "epoch 317/1000   error=0.011427\n",
      "epoch 318/1000   error=0.011005\n",
      "epoch 319/1000   error=0.010609\n",
      "epoch 320/1000   error=0.010235\n",
      "epoch 321/1000   error=0.009883\n",
      "epoch 322/1000   error=0.009551\n",
      "epoch 323/1000   error=0.009237\n",
      "epoch 324/1000   error=0.008940\n",
      "epoch 325/1000   error=0.008659\n",
      "epoch 326/1000   error=0.008393\n",
      "epoch 327/1000   error=0.008141\n",
      "epoch 328/1000   error=0.007901\n",
      "epoch 329/1000   error=0.007673\n",
      "epoch 330/1000   error=0.007456\n",
      "epoch 331/1000   error=0.007249\n",
      "epoch 332/1000   error=0.007052\n",
      "epoch 333/1000   error=0.006864\n",
      "epoch 334/1000   error=0.006685\n",
      "epoch 335/1000   error=0.006514\n",
      "epoch 336/1000   error=0.006350\n",
      "epoch 337/1000   error=0.006193\n",
      "epoch 338/1000   error=0.006043\n",
      "epoch 339/1000   error=0.005899\n",
      "epoch 340/1000   error=0.005761\n",
      "epoch 341/1000   error=0.005628\n",
      "epoch 342/1000   error=0.005501\n",
      "epoch 343/1000   error=0.005379\n",
      "epoch 344/1000   error=0.005261\n",
      "epoch 345/1000   error=0.005148\n",
      "epoch 346/1000   error=0.005040\n",
      "epoch 347/1000   error=0.004935\n",
      "epoch 348/1000   error=0.004834\n",
      "epoch 349/1000   error=0.004736\n",
      "epoch 350/1000   error=0.004642\n",
      "epoch 351/1000   error=0.004551\n",
      "epoch 352/1000   error=0.004464\n",
      "epoch 353/1000   error=0.004379\n",
      "epoch 354/1000   error=0.004297\n",
      "epoch 355/1000   error=0.004218\n",
      "epoch 356/1000   error=0.004141\n",
      "epoch 357/1000   error=0.004067\n",
      "epoch 358/1000   error=0.003995\n",
      "epoch 359/1000   error=0.003926\n",
      "epoch 360/1000   error=0.003858\n",
      "epoch 361/1000   error=0.003793\n",
      "epoch 362/1000   error=0.003730\n",
      "epoch 363/1000   error=0.003668\n",
      "epoch 364/1000   error=0.003608\n",
      "epoch 365/1000   error=0.003550\n",
      "epoch 366/1000   error=0.003494\n",
      "epoch 367/1000   error=0.003439\n",
      "epoch 368/1000   error=0.003386\n",
      "epoch 369/1000   error=0.003334\n",
      "epoch 370/1000   error=0.003284\n",
      "epoch 371/1000   error=0.003235\n",
      "epoch 372/1000   error=0.003187\n",
      "epoch 373/1000   error=0.003140\n",
      "epoch 374/1000   error=0.003095\n",
      "epoch 375/1000   error=0.003051\n",
      "epoch 376/1000   error=0.003008\n",
      "epoch 377/1000   error=0.002966\n",
      "epoch 378/1000   error=0.002925\n",
      "epoch 379/1000   error=0.002886\n",
      "epoch 380/1000   error=0.002847\n",
      "epoch 381/1000   error=0.002809\n",
      "epoch 382/1000   error=0.002772\n",
      "epoch 383/1000   error=0.002736\n",
      "epoch 384/1000   error=0.002700\n",
      "epoch 385/1000   error=0.002666\n",
      "epoch 386/1000   error=0.002632\n",
      "epoch 387/1000   error=0.002599\n",
      "epoch 388/1000   error=0.002567\n",
      "epoch 389/1000   error=0.002536\n",
      "epoch 390/1000   error=0.002505\n",
      "epoch 391/1000   error=0.002475\n",
      "epoch 392/1000   error=0.002446\n",
      "epoch 393/1000   error=0.002417\n",
      "epoch 394/1000   error=0.002389\n",
      "epoch 395/1000   error=0.002361\n",
      "epoch 396/1000   error=0.002334\n",
      "epoch 397/1000   error=0.002308\n",
      "epoch 398/1000   error=0.002282\n",
      "epoch 399/1000   error=0.002256\n",
      "epoch 400/1000   error=0.002231\n",
      "epoch 401/1000   error=0.002207\n",
      "epoch 402/1000   error=0.002183\n",
      "epoch 403/1000   error=0.002160\n",
      "epoch 404/1000   error=0.002137\n",
      "epoch 405/1000   error=0.002114\n",
      "epoch 406/1000   error=0.002092\n",
      "epoch 407/1000   error=0.002071\n",
      "epoch 408/1000   error=0.002049\n",
      "epoch 409/1000   error=0.002028\n",
      "epoch 410/1000   error=0.002008\n",
      "epoch 411/1000   error=0.001988\n",
      "epoch 412/1000   error=0.001968\n",
      "epoch 413/1000   error=0.001949\n",
      "epoch 414/1000   error=0.001930\n",
      "epoch 415/1000   error=0.001911\n",
      "epoch 416/1000   error=0.001893\n",
      "epoch 417/1000   error=0.001875\n",
      "epoch 418/1000   error=0.001857\n",
      "epoch 419/1000   error=0.001839\n",
      "epoch 420/1000   error=0.001822\n",
      "epoch 421/1000   error=0.001806\n",
      "epoch 422/1000   error=0.001789\n",
      "epoch 423/1000   error=0.001773\n",
      "epoch 424/1000   error=0.001757\n",
      "epoch 425/1000   error=0.001741\n",
      "epoch 426/1000   error=0.001726\n",
      "epoch 427/1000   error=0.001710\n",
      "epoch 428/1000   error=0.001695\n",
      "epoch 429/1000   error=0.001681\n",
      "epoch 430/1000   error=0.001666\n",
      "epoch 431/1000   error=0.001652\n",
      "epoch 432/1000   error=0.001638\n",
      "epoch 433/1000   error=0.001624\n",
      "epoch 434/1000   error=0.001610\n",
      "epoch 435/1000   error=0.001597\n",
      "epoch 436/1000   error=0.001584\n",
      "epoch 437/1000   error=0.001571\n",
      "epoch 438/1000   error=0.001558\n",
      "epoch 439/1000   error=0.001545\n",
      "epoch 440/1000   error=0.001533\n",
      "epoch 441/1000   error=0.001521\n",
      "epoch 442/1000   error=0.001509\n",
      "epoch 443/1000   error=0.001497\n",
      "epoch 444/1000   error=0.001485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 445/1000   error=0.001473\n",
      "epoch 446/1000   error=0.001462\n",
      "epoch 447/1000   error=0.001451\n",
      "epoch 448/1000   error=0.001440\n",
      "epoch 449/1000   error=0.001429\n",
      "epoch 450/1000   error=0.001418\n",
      "epoch 451/1000   error=0.001408\n",
      "epoch 452/1000   error=0.001397\n",
      "epoch 453/1000   error=0.001387\n",
      "epoch 454/1000   error=0.001377\n",
      "epoch 455/1000   error=0.001367\n",
      "epoch 456/1000   error=0.001357\n",
      "epoch 457/1000   error=0.001347\n",
      "epoch 458/1000   error=0.001337\n",
      "epoch 459/1000   error=0.001328\n",
      "epoch 460/1000   error=0.001318\n",
      "epoch 461/1000   error=0.001309\n",
      "epoch 462/1000   error=0.001300\n",
      "epoch 463/1000   error=0.001291\n",
      "epoch 464/1000   error=0.001282\n",
      "epoch 465/1000   error=0.001273\n",
      "epoch 466/1000   error=0.001264\n",
      "epoch 467/1000   error=0.001256\n",
      "epoch 468/1000   error=0.001247\n",
      "epoch 469/1000   error=0.001239\n",
      "epoch 470/1000   error=0.001231\n",
      "epoch 471/1000   error=0.001223\n",
      "epoch 472/1000   error=0.001215\n",
      "epoch 473/1000   error=0.001207\n",
      "epoch 474/1000   error=0.001199\n",
      "epoch 475/1000   error=0.001191\n",
      "epoch 476/1000   error=0.001183\n",
      "epoch 477/1000   error=0.001176\n",
      "epoch 478/1000   error=0.001168\n",
      "epoch 479/1000   error=0.001161\n",
      "epoch 480/1000   error=0.001154\n",
      "epoch 481/1000   error=0.001146\n",
      "epoch 482/1000   error=0.001139\n",
      "epoch 483/1000   error=0.001132\n",
      "epoch 484/1000   error=0.001125\n",
      "epoch 485/1000   error=0.001118\n",
      "epoch 486/1000   error=0.001111\n",
      "epoch 487/1000   error=0.001105\n",
      "epoch 488/1000   error=0.001098\n",
      "epoch 489/1000   error=0.001091\n",
      "epoch 490/1000   error=0.001085\n",
      "epoch 491/1000   error=0.001078\n",
      "epoch 492/1000   error=0.001072\n",
      "epoch 493/1000   error=0.001066\n",
      "epoch 494/1000   error=0.001059\n",
      "epoch 495/1000   error=0.001053\n",
      "epoch 496/1000   error=0.001047\n",
      "epoch 497/1000   error=0.001041\n",
      "epoch 498/1000   error=0.001035\n",
      "epoch 499/1000   error=0.001029\n",
      "epoch 500/1000   error=0.001023\n",
      "epoch 501/1000   error=0.001018\n",
      "epoch 502/1000   error=0.001012\n",
      "epoch 503/1000   error=0.001006\n",
      "epoch 504/1000   error=0.001001\n",
      "epoch 505/1000   error=0.000995\n",
      "epoch 506/1000   error=0.000990\n",
      "epoch 507/1000   error=0.000984\n",
      "epoch 508/1000   error=0.000979\n",
      "epoch 509/1000   error=0.000973\n",
      "epoch 510/1000   error=0.000968\n",
      "epoch 511/1000   error=0.000963\n",
      "epoch 512/1000   error=0.000958\n",
      "epoch 513/1000   error=0.000953\n",
      "epoch 514/1000   error=0.000948\n",
      "epoch 515/1000   error=0.000943\n",
      "epoch 516/1000   error=0.000938\n",
      "epoch 517/1000   error=0.000933\n",
      "epoch 518/1000   error=0.000928\n",
      "epoch 519/1000   error=0.000923\n",
      "epoch 520/1000   error=0.000918\n",
      "epoch 521/1000   error=0.000913\n",
      "epoch 522/1000   error=0.000909\n",
      "epoch 523/1000   error=0.000904\n",
      "epoch 524/1000   error=0.000900\n",
      "epoch 525/1000   error=0.000895\n",
      "epoch 526/1000   error=0.000890\n",
      "epoch 527/1000   error=0.000886\n",
      "epoch 528/1000   error=0.000882\n",
      "epoch 529/1000   error=0.000877\n",
      "epoch 530/1000   error=0.000873\n",
      "epoch 531/1000   error=0.000869\n",
      "epoch 532/1000   error=0.000864\n",
      "epoch 533/1000   error=0.000860\n",
      "epoch 534/1000   error=0.000856\n",
      "epoch 535/1000   error=0.000852\n",
      "epoch 536/1000   error=0.000848\n",
      "epoch 537/1000   error=0.000844\n",
      "epoch 538/1000   error=0.000840\n",
      "epoch 539/1000   error=0.000836\n",
      "epoch 540/1000   error=0.000832\n",
      "epoch 541/1000   error=0.000828\n",
      "epoch 542/1000   error=0.000824\n",
      "epoch 543/1000   error=0.000820\n",
      "epoch 544/1000   error=0.000816\n",
      "epoch 545/1000   error=0.000812\n",
      "epoch 546/1000   error=0.000808\n",
      "epoch 547/1000   error=0.000805\n",
      "epoch 548/1000   error=0.000801\n",
      "epoch 549/1000   error=0.000797\n",
      "epoch 550/1000   error=0.000794\n",
      "epoch 551/1000   error=0.000790\n",
      "epoch 552/1000   error=0.000787\n",
      "epoch 553/1000   error=0.000783\n",
      "epoch 554/1000   error=0.000780\n",
      "epoch 555/1000   error=0.000776\n",
      "epoch 556/1000   error=0.000773\n",
      "epoch 557/1000   error=0.000769\n",
      "epoch 558/1000   error=0.000766\n",
      "epoch 559/1000   error=0.000762\n",
      "epoch 560/1000   error=0.000759\n",
      "epoch 561/1000   error=0.000756\n",
      "epoch 562/1000   error=0.000752\n",
      "epoch 563/1000   error=0.000749\n",
      "epoch 564/1000   error=0.000746\n",
      "epoch 565/1000   error=0.000743\n",
      "epoch 566/1000   error=0.000740\n",
      "epoch 567/1000   error=0.000736\n",
      "epoch 568/1000   error=0.000733\n",
      "epoch 569/1000   error=0.000730\n",
      "epoch 570/1000   error=0.000727\n",
      "epoch 571/1000   error=0.000724\n",
      "epoch 572/1000   error=0.000721\n",
      "epoch 573/1000   error=0.000718\n",
      "epoch 574/1000   error=0.000715\n",
      "epoch 575/1000   error=0.000712\n",
      "epoch 576/1000   error=0.000709\n",
      "epoch 577/1000   error=0.000706\n",
      "epoch 578/1000   error=0.000703\n",
      "epoch 579/1000   error=0.000700\n",
      "epoch 580/1000   error=0.000698\n",
      "epoch 581/1000   error=0.000695\n",
      "epoch 582/1000   error=0.000692\n",
      "epoch 583/1000   error=0.000689\n",
      "epoch 584/1000   error=0.000686\n",
      "epoch 585/1000   error=0.000684\n",
      "epoch 586/1000   error=0.000681\n",
      "epoch 587/1000   error=0.000678\n",
      "epoch 588/1000   error=0.000676\n",
      "epoch 589/1000   error=0.000673\n",
      "epoch 590/1000   error=0.000670\n",
      "epoch 591/1000   error=0.000668\n",
      "epoch 592/1000   error=0.000665\n",
      "epoch 593/1000   error=0.000662\n",
      "epoch 594/1000   error=0.000660\n",
      "epoch 595/1000   error=0.000657\n",
      "epoch 596/1000   error=0.000655\n",
      "epoch 597/1000   error=0.000652\n",
      "epoch 598/1000   error=0.000650\n",
      "epoch 599/1000   error=0.000647\n",
      "epoch 600/1000   error=0.000645\n",
      "epoch 601/1000   error=0.000642\n",
      "epoch 602/1000   error=0.000640\n",
      "epoch 603/1000   error=0.000638\n",
      "epoch 604/1000   error=0.000635\n",
      "epoch 605/1000   error=0.000633\n",
      "epoch 606/1000   error=0.000630\n",
      "epoch 607/1000   error=0.000628\n",
      "epoch 608/1000   error=0.000626\n",
      "epoch 609/1000   error=0.000624\n",
      "epoch 610/1000   error=0.000621\n",
      "epoch 611/1000   error=0.000619\n",
      "epoch 612/1000   error=0.000617\n",
      "epoch 613/1000   error=0.000614\n",
      "epoch 614/1000   error=0.000612\n",
      "epoch 615/1000   error=0.000610\n",
      "epoch 616/1000   error=0.000608\n",
      "epoch 617/1000   error=0.000606\n",
      "epoch 618/1000   error=0.000603\n",
      "epoch 619/1000   error=0.000601\n",
      "epoch 620/1000   error=0.000599\n",
      "epoch 621/1000   error=0.000597\n",
      "epoch 622/1000   error=0.000595\n",
      "epoch 623/1000   error=0.000593\n",
      "epoch 624/1000   error=0.000591\n",
      "epoch 625/1000   error=0.000589\n",
      "epoch 626/1000   error=0.000587\n",
      "epoch 627/1000   error=0.000585\n",
      "epoch 628/1000   error=0.000583\n",
      "epoch 629/1000   error=0.000581\n",
      "epoch 630/1000   error=0.000579\n",
      "epoch 631/1000   error=0.000577\n",
      "epoch 632/1000   error=0.000575\n",
      "epoch 633/1000   error=0.000573\n",
      "epoch 634/1000   error=0.000571\n",
      "epoch 635/1000   error=0.000569\n",
      "epoch 636/1000   error=0.000567\n",
      "epoch 637/1000   error=0.000565\n",
      "epoch 638/1000   error=0.000563\n",
      "epoch 639/1000   error=0.000561\n",
      "epoch 640/1000   error=0.000559\n",
      "epoch 641/1000   error=0.000557\n",
      "epoch 642/1000   error=0.000555\n",
      "epoch 643/1000   error=0.000554\n",
      "epoch 644/1000   error=0.000552\n",
      "epoch 645/1000   error=0.000550\n",
      "epoch 646/1000   error=0.000548\n",
      "epoch 647/1000   error=0.000546\n",
      "epoch 648/1000   error=0.000545\n",
      "epoch 649/1000   error=0.000543\n",
      "epoch 650/1000   error=0.000541\n",
      "epoch 651/1000   error=0.000539\n",
      "epoch 652/1000   error=0.000538\n",
      "epoch 653/1000   error=0.000536\n",
      "epoch 654/1000   error=0.000534\n",
      "epoch 655/1000   error=0.000532\n",
      "epoch 656/1000   error=0.000531\n",
      "epoch 657/1000   error=0.000529\n",
      "epoch 658/1000   error=0.000527\n",
      "epoch 659/1000   error=0.000526\n",
      "epoch 660/1000   error=0.000524\n",
      "epoch 661/1000   error=0.000522\n",
      "epoch 662/1000   error=0.000521\n",
      "epoch 663/1000   error=0.000519\n",
      "epoch 664/1000   error=0.000517\n",
      "epoch 665/1000   error=0.000516\n",
      "epoch 666/1000   error=0.000514\n",
      "epoch 667/1000   error=0.000513\n",
      "epoch 668/1000   error=0.000511\n",
      "epoch 669/1000   error=0.000509\n",
      "epoch 670/1000   error=0.000508\n",
      "epoch 671/1000   error=0.000506\n",
      "epoch 672/1000   error=0.000505\n",
      "epoch 673/1000   error=0.000503\n",
      "epoch 674/1000   error=0.000502\n",
      "epoch 675/1000   error=0.000500\n",
      "epoch 676/1000   error=0.000499\n",
      "epoch 677/1000   error=0.000497\n",
      "epoch 678/1000   error=0.000496\n",
      "epoch 679/1000   error=0.000494\n",
      "epoch 680/1000   error=0.000493\n",
      "epoch 681/1000   error=0.000491\n",
      "epoch 682/1000   error=0.000490\n",
      "epoch 683/1000   error=0.000488\n",
      "epoch 684/1000   error=0.000487\n",
      "epoch 685/1000   error=0.000485\n",
      "epoch 686/1000   error=0.000484\n",
      "epoch 687/1000   error=0.000483\n",
      "epoch 688/1000   error=0.000481\n",
      "epoch 689/1000   error=0.000480\n",
      "epoch 690/1000   error=0.000478\n",
      "epoch 691/1000   error=0.000477\n",
      "epoch 692/1000   error=0.000476\n",
      "epoch 693/1000   error=0.000474\n",
      "epoch 694/1000   error=0.000473\n",
      "epoch 695/1000   error=0.000472\n",
      "epoch 696/1000   error=0.000470\n",
      "epoch 697/1000   error=0.000469\n",
      "epoch 698/1000   error=0.000467\n",
      "epoch 699/1000   error=0.000466\n",
      "epoch 700/1000   error=0.000465\n",
      "epoch 701/1000   error=0.000464\n",
      "epoch 702/1000   error=0.000462\n",
      "epoch 703/1000   error=0.000461\n",
      "epoch 704/1000   error=0.000460\n",
      "epoch 705/1000   error=0.000458\n",
      "epoch 706/1000   error=0.000457\n",
      "epoch 707/1000   error=0.000456\n",
      "epoch 708/1000   error=0.000454\n",
      "epoch 709/1000   error=0.000453\n",
      "epoch 710/1000   error=0.000452\n",
      "epoch 711/1000   error=0.000451\n",
      "epoch 712/1000   error=0.000449\n",
      "epoch 713/1000   error=0.000448\n",
      "epoch 714/1000   error=0.000447\n",
      "epoch 715/1000   error=0.000446\n",
      "epoch 716/1000   error=0.000445\n",
      "epoch 717/1000   error=0.000443\n",
      "epoch 718/1000   error=0.000442\n",
      "epoch 719/1000   error=0.000441\n",
      "epoch 720/1000   error=0.000440\n",
      "epoch 721/1000   error=0.000439\n",
      "epoch 722/1000   error=0.000437\n",
      "epoch 723/1000   error=0.000436\n",
      "epoch 724/1000   error=0.000435\n",
      "epoch 725/1000   error=0.000434\n",
      "epoch 726/1000   error=0.000433\n",
      "epoch 727/1000   error=0.000432\n",
      "epoch 728/1000   error=0.000430\n",
      "epoch 729/1000   error=0.000429\n",
      "epoch 730/1000   error=0.000428\n",
      "epoch 731/1000   error=0.000427\n",
      "epoch 732/1000   error=0.000426\n",
      "epoch 733/1000   error=0.000425\n",
      "epoch 734/1000   error=0.000424\n",
      "epoch 735/1000   error=0.000423\n",
      "epoch 736/1000   error=0.000421\n",
      "epoch 737/1000   error=0.000420\n",
      "epoch 738/1000   error=0.000419\n",
      "epoch 739/1000   error=0.000418\n",
      "epoch 740/1000   error=0.000417\n",
      "epoch 741/1000   error=0.000416\n",
      "epoch 742/1000   error=0.000415\n",
      "epoch 743/1000   error=0.000414\n",
      "epoch 744/1000   error=0.000413\n",
      "epoch 745/1000   error=0.000412\n",
      "epoch 746/1000   error=0.000411\n",
      "epoch 747/1000   error=0.000410\n",
      "epoch 748/1000   error=0.000409\n",
      "epoch 749/1000   error=0.000408\n",
      "epoch 750/1000   error=0.000407\n",
      "epoch 751/1000   error=0.000406\n",
      "epoch 752/1000   error=0.000405\n",
      "epoch 753/1000   error=0.000404\n",
      "epoch 754/1000   error=0.000403\n",
      "epoch 755/1000   error=0.000402\n",
      "epoch 756/1000   error=0.000401\n",
      "epoch 757/1000   error=0.000400\n",
      "epoch 758/1000   error=0.000399\n",
      "epoch 759/1000   error=0.000398\n",
      "epoch 760/1000   error=0.000397\n",
      "epoch 761/1000   error=0.000396\n",
      "epoch 762/1000   error=0.000395\n",
      "epoch 763/1000   error=0.000394\n",
      "epoch 764/1000   error=0.000393\n",
      "epoch 765/1000   error=0.000392\n",
      "epoch 766/1000   error=0.000391\n",
      "epoch 767/1000   error=0.000390\n",
      "epoch 768/1000   error=0.000389\n",
      "epoch 769/1000   error=0.000388\n",
      "epoch 770/1000   error=0.000387\n",
      "epoch 771/1000   error=0.000386\n",
      "epoch 772/1000   error=0.000385\n",
      "epoch 773/1000   error=0.000384\n",
      "epoch 774/1000   error=0.000383\n",
      "epoch 775/1000   error=0.000382\n",
      "epoch 776/1000   error=0.000382\n",
      "epoch 777/1000   error=0.000381\n",
      "epoch 778/1000   error=0.000380\n",
      "epoch 779/1000   error=0.000379\n",
      "epoch 780/1000   error=0.000378\n",
      "epoch 781/1000   error=0.000377\n",
      "epoch 782/1000   error=0.000376\n",
      "epoch 783/1000   error=0.000375\n",
      "epoch 784/1000   error=0.000374\n",
      "epoch 785/1000   error=0.000374\n",
      "epoch 786/1000   error=0.000373\n",
      "epoch 787/1000   error=0.000372\n",
      "epoch 788/1000   error=0.000371\n",
      "epoch 789/1000   error=0.000370\n",
      "epoch 790/1000   error=0.000369\n",
      "epoch 791/1000   error=0.000368\n",
      "epoch 792/1000   error=0.000368\n",
      "epoch 793/1000   error=0.000367\n",
      "epoch 794/1000   error=0.000366\n",
      "epoch 795/1000   error=0.000365\n",
      "epoch 796/1000   error=0.000364\n",
      "epoch 797/1000   error=0.000363\n",
      "epoch 798/1000   error=0.000363\n",
      "epoch 799/1000   error=0.000362\n",
      "epoch 800/1000   error=0.000361\n",
      "epoch 801/1000   error=0.000360\n",
      "epoch 802/1000   error=0.000359\n",
      "epoch 803/1000   error=0.000358\n",
      "epoch 804/1000   error=0.000358\n",
      "epoch 805/1000   error=0.000357\n",
      "epoch 806/1000   error=0.000356\n",
      "epoch 807/1000   error=0.000355\n",
      "epoch 808/1000   error=0.000354\n",
      "epoch 809/1000   error=0.000354\n",
      "epoch 810/1000   error=0.000353\n",
      "epoch 811/1000   error=0.000352\n",
      "epoch 812/1000   error=0.000351\n",
      "epoch 813/1000   error=0.000351\n",
      "epoch 814/1000   error=0.000350\n",
      "epoch 815/1000   error=0.000349\n",
      "epoch 816/1000   error=0.000348\n",
      "epoch 817/1000   error=0.000347\n",
      "epoch 818/1000   error=0.000347\n",
      "epoch 819/1000   error=0.000346\n",
      "epoch 820/1000   error=0.000345\n",
      "epoch 821/1000   error=0.000344\n",
      "epoch 822/1000   error=0.000344\n",
      "epoch 823/1000   error=0.000343\n",
      "epoch 824/1000   error=0.000342\n",
      "epoch 825/1000   error=0.000341\n",
      "epoch 826/1000   error=0.000341\n",
      "epoch 827/1000   error=0.000340\n",
      "epoch 828/1000   error=0.000339\n",
      "epoch 829/1000   error=0.000339\n",
      "epoch 830/1000   error=0.000338\n",
      "epoch 831/1000   error=0.000337\n",
      "epoch 832/1000   error=0.000336\n",
      "epoch 833/1000   error=0.000336\n",
      "epoch 834/1000   error=0.000335\n",
      "epoch 835/1000   error=0.000334\n",
      "epoch 836/1000   error=0.000334\n",
      "epoch 837/1000   error=0.000333\n",
      "epoch 838/1000   error=0.000332\n",
      "epoch 839/1000   error=0.000331\n",
      "epoch 840/1000   error=0.000331\n",
      "epoch 841/1000   error=0.000330\n",
      "epoch 842/1000   error=0.000329\n",
      "epoch 843/1000   error=0.000329\n",
      "epoch 844/1000   error=0.000328\n",
      "epoch 845/1000   error=0.000327\n",
      "epoch 846/1000   error=0.000327\n",
      "epoch 847/1000   error=0.000326\n",
      "epoch 848/1000   error=0.000325\n",
      "epoch 849/1000   error=0.000325\n",
      "epoch 850/1000   error=0.000324\n",
      "epoch 851/1000   error=0.000323\n",
      "epoch 852/1000   error=0.000323\n",
      "epoch 853/1000   error=0.000322\n",
      "epoch 854/1000   error=0.000321\n",
      "epoch 855/1000   error=0.000321\n",
      "epoch 856/1000   error=0.000320\n",
      "epoch 857/1000   error=0.000319\n",
      "epoch 858/1000   error=0.000319\n",
      "epoch 859/1000   error=0.000318\n",
      "epoch 860/1000   error=0.000317\n",
      "epoch 861/1000   error=0.000317\n",
      "epoch 862/1000   error=0.000316\n",
      "epoch 863/1000   error=0.000316\n",
      "epoch 864/1000   error=0.000315\n",
      "epoch 865/1000   error=0.000314\n",
      "epoch 866/1000   error=0.000314\n",
      "epoch 867/1000   error=0.000313\n",
      "epoch 868/1000   error=0.000312\n",
      "epoch 869/1000   error=0.000312\n",
      "epoch 870/1000   error=0.000311\n",
      "epoch 871/1000   error=0.000311\n",
      "epoch 872/1000   error=0.000310\n",
      "epoch 873/1000   error=0.000309\n",
      "epoch 874/1000   error=0.000309\n",
      "epoch 875/1000   error=0.000308\n",
      "epoch 876/1000   error=0.000307\n",
      "epoch 877/1000   error=0.000307\n",
      "epoch 878/1000   error=0.000306\n",
      "epoch 879/1000   error=0.000306\n",
      "epoch 880/1000   error=0.000305\n",
      "epoch 881/1000   error=0.000304\n",
      "epoch 882/1000   error=0.000304\n",
      "epoch 883/1000   error=0.000303\n",
      "epoch 884/1000   error=0.000303\n",
      "epoch 885/1000   error=0.000302\n",
      "epoch 886/1000   error=0.000302\n",
      "epoch 887/1000   error=0.000301\n",
      "epoch 888/1000   error=0.000300\n",
      "epoch 889/1000   error=0.000300\n",
      "epoch 890/1000   error=0.000299\n",
      "epoch 891/1000   error=0.000299\n",
      "epoch 892/1000   error=0.000298\n",
      "epoch 893/1000   error=0.000298\n",
      "epoch 894/1000   error=0.000297\n",
      "epoch 895/1000   error=0.000296\n",
      "epoch 896/1000   error=0.000296\n",
      "epoch 897/1000   error=0.000295\n",
      "epoch 898/1000   error=0.000295\n",
      "epoch 899/1000   error=0.000294\n",
      "epoch 900/1000   error=0.000294\n",
      "epoch 901/1000   error=0.000293\n",
      "epoch 902/1000   error=0.000293\n",
      "epoch 903/1000   error=0.000292\n",
      "epoch 904/1000   error=0.000291\n",
      "epoch 905/1000   error=0.000291\n",
      "epoch 906/1000   error=0.000290\n",
      "epoch 907/1000   error=0.000290\n",
      "epoch 908/1000   error=0.000289\n",
      "epoch 909/1000   error=0.000289\n",
      "epoch 910/1000   error=0.000288\n",
      "epoch 911/1000   error=0.000288\n",
      "epoch 912/1000   error=0.000287\n",
      "epoch 913/1000   error=0.000287\n",
      "epoch 914/1000   error=0.000286\n",
      "epoch 915/1000   error=0.000286\n",
      "epoch 916/1000   error=0.000285\n",
      "epoch 917/1000   error=0.000285\n",
      "epoch 918/1000   error=0.000284\n",
      "epoch 919/1000   error=0.000283\n",
      "epoch 920/1000   error=0.000283\n",
      "epoch 921/1000   error=0.000282\n",
      "epoch 922/1000   error=0.000282\n",
      "epoch 923/1000   error=0.000281\n",
      "epoch 924/1000   error=0.000281\n",
      "epoch 925/1000   error=0.000280\n",
      "epoch 926/1000   error=0.000280\n",
      "epoch 927/1000   error=0.000279\n",
      "epoch 928/1000   error=0.000279\n",
      "epoch 929/1000   error=0.000278\n",
      "epoch 930/1000   error=0.000278\n",
      "epoch 931/1000   error=0.000277\n",
      "epoch 932/1000   error=0.000277\n",
      "epoch 933/1000   error=0.000276\n",
      "epoch 934/1000   error=0.000276\n",
      "epoch 935/1000   error=0.000275\n",
      "epoch 936/1000   error=0.000275\n",
      "epoch 937/1000   error=0.000274\n",
      "epoch 938/1000   error=0.000274\n",
      "epoch 939/1000   error=0.000273\n",
      "epoch 940/1000   error=0.000273\n",
      "epoch 941/1000   error=0.000273\n",
      "epoch 942/1000   error=0.000272\n",
      "epoch 943/1000   error=0.000272\n",
      "epoch 944/1000   error=0.000271\n",
      "epoch 945/1000   error=0.000271\n",
      "epoch 946/1000   error=0.000270\n",
      "epoch 947/1000   error=0.000270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 948/1000   error=0.000269\n",
      "epoch 949/1000   error=0.000269\n",
      "epoch 950/1000   error=0.000268\n",
      "epoch 951/1000   error=0.000268\n",
      "epoch 952/1000   error=0.000267\n",
      "epoch 953/1000   error=0.000267\n",
      "epoch 954/1000   error=0.000266\n",
      "epoch 955/1000   error=0.000266\n",
      "epoch 956/1000   error=0.000266\n",
      "epoch 957/1000   error=0.000265\n",
      "epoch 958/1000   error=0.000265\n",
      "epoch 959/1000   error=0.000264\n",
      "epoch 960/1000   error=0.000264\n",
      "epoch 961/1000   error=0.000263\n",
      "epoch 962/1000   error=0.000263\n",
      "epoch 963/1000   error=0.000262\n",
      "epoch 964/1000   error=0.000262\n",
      "epoch 965/1000   error=0.000261\n",
      "epoch 966/1000   error=0.000261\n",
      "epoch 967/1000   error=0.000261\n",
      "epoch 968/1000   error=0.000260\n",
      "epoch 969/1000   error=0.000260\n",
      "epoch 970/1000   error=0.000259\n",
      "epoch 971/1000   error=0.000259\n",
      "epoch 972/1000   error=0.000258\n",
      "epoch 973/1000   error=0.000258\n",
      "epoch 974/1000   error=0.000258\n",
      "epoch 975/1000   error=0.000257\n",
      "epoch 976/1000   error=0.000257\n",
      "epoch 977/1000   error=0.000256\n",
      "epoch 978/1000   error=0.000256\n",
      "epoch 979/1000   error=0.000255\n",
      "epoch 980/1000   error=0.000255\n",
      "epoch 981/1000   error=0.000255\n",
      "epoch 982/1000   error=0.000254\n",
      "epoch 983/1000   error=0.000254\n",
      "epoch 984/1000   error=0.000253\n",
      "epoch 985/1000   error=0.000253\n",
      "epoch 986/1000   error=0.000252\n",
      "epoch 987/1000   error=0.000252\n",
      "epoch 988/1000   error=0.000252\n",
      "epoch 989/1000   error=0.000251\n",
      "epoch 990/1000   error=0.000251\n",
      "epoch 991/1000   error=0.000250\n",
      "epoch 992/1000   error=0.000250\n",
      "epoch 993/1000   error=0.000250\n",
      "epoch 994/1000   error=0.000249\n",
      "epoch 995/1000   error=0.000249\n",
      "epoch 996/1000   error=0.000248\n",
      "epoch 997/1000   error=0.000248\n",
      "epoch 998/1000   error=0.000248\n",
      "epoch 999/1000   error=0.000247\n",
      "epoch 1000/1000   error=0.000247\n",
      "[array([[0.00114992]]), array([[0.9779149]]), array([[0.97777017]]), array([[-0.00035612]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#from network import Network\n",
    "#from fc_layer import FCLayer\n",
    "#from activation_layer import ActivationLayer\n",
    "#from activations import tanh, tanh_prime\n",
    "#from losses import mse, mse_prime\n",
    "\n",
    "# training data\n",
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "# network\n",
    "net = Network()\n",
    "net.add(FCLayer(2, 3))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(3, 1))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# test\n",
    "out = net.predict(x_train)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo6R7DeFBGve"
   },
   "source": [
    "### Solve MNIST\n",
    "We didn’t implemented the Convolutional Layer but this is not a problem. \n",
    "All we need to do is to reshape our data so that it can fit into a Fully Connected Layer.\n",
    "MNIST Dataset consists of images of digits from 0 to 9, of shape 28x28x1. \n",
    "The goal is to predict what digit is drawn on a picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NP9YuTSBGve",
    "outputId": "c33d6a40-42a8-4855-8324-e4b52bacf5b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/35   error=0.267268\n",
      "epoch 2/35   error=0.136776\n",
      "epoch 3/35   error=0.129875\n",
      "epoch 4/35   error=0.122785\n",
      "epoch 5/35   error=0.119283\n",
      "epoch 6/35   error=0.116729\n",
      "epoch 7/35   error=0.116644\n",
      "epoch 8/35   error=0.112951\n",
      "epoch 9/35   error=0.109218\n",
      "epoch 10/35   error=0.111558\n",
      "epoch 11/35   error=0.109720\n",
      "epoch 12/35   error=0.108266\n",
      "epoch 13/35   error=0.108011\n",
      "epoch 14/35   error=0.106802\n",
      "epoch 15/35   error=0.105264\n",
      "epoch 16/35   error=0.105535\n",
      "epoch 17/35   error=0.104213\n",
      "epoch 18/35   error=0.103155\n",
      "epoch 19/35   error=0.107140\n",
      "epoch 20/35   error=0.106981\n",
      "epoch 21/35   error=0.106873\n",
      "epoch 22/35   error=0.105653\n",
      "epoch 23/35   error=0.105620\n",
      "epoch 24/35   error=0.102567\n",
      "epoch 25/35   error=0.099447\n",
      "epoch 26/35   error=0.098495\n",
      "epoch 27/35   error=0.097795\n",
      "epoch 28/35   error=0.097155\n",
      "epoch 29/35   error=0.096501\n",
      "epoch 30/35   error=0.095805\n",
      "epoch 31/35   error=0.095044\n",
      "epoch 32/35   error=0.094211\n",
      "epoch 33/35   error=0.093738\n",
      "epoch 34/35   error=0.093314\n",
      "epoch 35/35   error=0.093224\n",
      "\n",
      "\n",
      "predicted values : \n",
      "[array([[ 0.13753229,  0.01668488, -0.06607155,  0.12643139, -0.08889765,\n",
      "        -0.18358105,  0.30831152,  0.76610774, -0.098793  ,  0.00496229]]), array([[ 0.10135665,  0.23995254,  0.13692553,  0.23278927, -0.08325186,\n",
      "         0.07141838,  0.26445969, -0.006742  ,  0.06684494,  0.07380391]]), array([[-0.24274058,  0.8988979 , -0.12338668, -0.08547863, -0.12921751,\n",
      "         0.05218579,  0.51914964, -0.00783749,  0.03050178,  0.19115139]])]\n",
      "true values : \n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#from network import Network\n",
    "#from fc_layer import FCLayer\n",
    "#from activation_layer import ActivationLayer\n",
    "#from activations import tanh, tanh_prime\n",
    "#from losses import mse, mse_prime\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train on 1000 samples\n",
    "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(x_test[0:3])\n",
    "print(\"\\n\")\n",
    "print(\"predicted values : \")\n",
    "print(out, end=\"\\n\")\n",
    "print(\"true values : \")\n",
    "print(y_test[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QC3LOWyYKqJ5",
    "outputId": "201be0de-c6b0-4e78-ff62-178eeb103186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.37      0.49       980\n",
      "           1       0.89      0.55      0.68      1135\n",
      "           2       0.20      0.00      0.00      1032\n",
      "           3       0.46      0.17      0.25      1010\n",
      "           4       0.63      0.14      0.22       982\n",
      "           5       0.43      0.11      0.17       892\n",
      "           6       0.12      0.90      0.21       958\n",
      "           7       0.67      0.53      0.59      1028\n",
      "           8       0.00      0.00      0.00       974\n",
      "           9       0.45      0.02      0.04      1009\n",
      "\n",
      "    accuracy                           0.28     10000\n",
      "   macro avg       0.46      0.28      0.27     10000\n",
      "weighted avg       0.47      0.28      0.27     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\340\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\340\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\340\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = [np.argmax(net.predict(x)) for x in x_test]\n",
    "y_true = [np.argmax(y) for y in y_test]\n",
    "print(classification_report(y_true, y_pred))\n",
    "#default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tV2_eq39EVx6"
   },
   "source": [
    "############################################\n",
    "\n",
    "**What can go wrong if we have a wide range of numbers in our input/output data and we don't do any pre-processing on them and feed the neural network with unprocessed data?**\n",
    "\n",
    "Since we multiply the input values on each layer (coming from multiple neurons), we may end up with very large numbers flowing in the network in forward propagation.\n",
    "############################################\n",
    "\n",
    "**How do we tackle this problem?**\n",
    "\n",
    "\n",
    "For tackling we can use or Standardization or Normalization .\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNODZ6guEpL3"
   },
   "source": [
    "#Normalization and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tvx9ZHaTENct"
   },
   "outputs": [],
   "source": [
    "def normalize(values):\n",
    "    return (values - values.min())/(values.max() - values.min())\n",
    "\n",
    "def standardize(values):\n",
    "    return (values - values.mean())/values.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ldsO1WqBGve",
    "outputId": "07e90ce6-4038-45f2-bcd8-246895ff8e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/35   error=0.240837\n",
      "epoch 2/35   error=0.100509\n",
      "epoch 3/35   error=0.078942\n",
      "epoch 4/35   error=0.065168\n",
      "epoch 5/35   error=0.055041\n",
      "epoch 6/35   error=0.047132\n",
      "epoch 7/35   error=0.041448\n",
      "epoch 8/35   error=0.036604\n",
      "epoch 9/35   error=0.032670\n",
      "epoch 10/35   error=0.029393\n",
      "epoch 11/35   error=0.026591\n",
      "epoch 12/35   error=0.024198\n",
      "epoch 13/35   error=0.022133\n",
      "epoch 14/35   error=0.020489\n",
      "epoch 15/35   error=0.019158\n",
      "epoch 16/35   error=0.018067\n",
      "epoch 17/35   error=0.017059\n",
      "epoch 18/35   error=0.016011\n",
      "epoch 19/35   error=0.015106\n",
      "epoch 20/35   error=0.014328\n",
      "epoch 21/35   error=0.013688\n",
      "epoch 22/35   error=0.013086\n",
      "epoch 23/35   error=0.012565\n",
      "epoch 24/35   error=0.012083\n",
      "epoch 25/35   error=0.011619\n",
      "epoch 26/35   error=0.011149\n",
      "epoch 27/35   error=0.010681\n",
      "epoch 28/35   error=0.010318\n",
      "epoch 29/35   error=0.009977\n",
      "epoch 30/35   error=0.009596\n",
      "epoch 31/35   error=0.009314\n",
      "epoch 32/35   error=0.009012\n",
      "epoch 33/35   error=0.008774\n",
      "epoch 34/35   error=0.008383\n",
      "epoch 35/35   error=0.008020\n",
      "\n",
      "\n",
      "predicted values : \n",
      "[array([[ 0.00290253, -0.00794452, -0.03193876,  0.00332988, -0.00951859,\n",
      "        -0.15366036, -0.00573053,  0.96686348, -0.02884201, -0.00681877]]), array([[ 0.01217485, -0.00321975,  0.87102726,  0.19431864, -0.04965051,\n",
      "         0.16985994,  0.09169788,  0.07435708,  0.20891833, -0.22373512]]), array([[-0.01042426,  0.9716923 , -0.05428947,  0.01034714,  0.02286798,\n",
      "        -0.01931841,  0.02605823,  0.0091377 , -0.01693906,  0.02296813]])]\n",
      "true values : \n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#from network import Network\n",
    "#from fc_layer import FCLayer\n",
    "#from activation_layer import ActivationLayer\n",
    "#from activations import tanh, tanh_prime\n",
    "#from losses import mse, mse_prime\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "x_train = normalize(x_train)\n",
    "x_test = normalize(x_test)\n",
    "\n",
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train on 1000 samples\n",
    "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(x_test[0:3])\n",
    "print(\"\\n\")\n",
    "print(\"predicted values : \")\n",
    "print(out, end=\"\\n\")\n",
    "print(\"true values : \")\n",
    "print(y_test[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mdcu7N3gBGvf",
    "outputId": "c1c6db7f-4b68-49d8-dac2-36428b170ce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       980\n",
      "           1       0.97      0.94      0.96      1135\n",
      "           2       0.86      0.77      0.81      1032\n",
      "           3       0.84      0.70      0.76      1010\n",
      "           4       0.82      0.72      0.77       982\n",
      "           5       0.59      0.70      0.64       892\n",
      "           6       0.88      0.76      0.82       958\n",
      "           7       0.82      0.86      0.84      1028\n",
      "           8       0.64      0.66      0.65       974\n",
      "           9       0.60      0.81      0.69      1009\n",
      "\n",
      "    accuracy                           0.79     10000\n",
      "   macro avg       0.80      0.78      0.79     10000\n",
      "weighted avg       0.80      0.79      0.79     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [np.argmax(net.predict(x)) for x in x_test]\n",
    "y_true = [np.argmax(y) for y in y_test]\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "#normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDlfutb_KrXu",
    "outputId": "fd7e7f16-9908-4e6d-f265-f878a756a646"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/35   error=0.271621\n",
      "epoch 2/35   error=0.095203\n",
      "epoch 3/35   error=0.072048\n",
      "epoch 4/35   error=0.060018\n",
      "epoch 5/35   error=0.052130\n",
      "epoch 6/35   error=0.045874\n",
      "epoch 7/35   error=0.041136\n",
      "epoch 8/35   error=0.036956\n",
      "epoch 9/35   error=0.033621\n",
      "epoch 10/35   error=0.030949\n",
      "epoch 11/35   error=0.028516\n",
      "epoch 12/35   error=0.026502\n",
      "epoch 13/35   error=0.024757\n",
      "epoch 14/35   error=0.023170\n",
      "epoch 15/35   error=0.021567\n",
      "epoch 16/35   error=0.020265\n",
      "epoch 17/35   error=0.019056\n",
      "epoch 18/35   error=0.018134\n",
      "epoch 19/35   error=0.016873\n",
      "epoch 20/35   error=0.016035\n",
      "epoch 21/35   error=0.015182\n",
      "epoch 22/35   error=0.014516\n",
      "epoch 23/35   error=0.013961\n",
      "epoch 24/35   error=0.013377\n",
      "epoch 25/35   error=0.012923\n",
      "epoch 26/35   error=0.012416\n",
      "epoch 27/35   error=0.011994\n",
      "epoch 28/35   error=0.011570\n",
      "epoch 29/35   error=0.011210\n",
      "epoch 30/35   error=0.010925\n",
      "epoch 31/35   error=0.010611\n",
      "epoch 32/35   error=0.010295\n",
      "epoch 33/35   error=0.009839\n",
      "epoch 34/35   error=0.009542\n",
      "epoch 35/35   error=0.009299\n",
      "\n",
      "\n",
      "predicted values : \n",
      "[array([[ 0.21170056, -0.02402761, -0.1364716 ,  0.57417355,  0.20925951,\n",
      "        -0.09051105, -0.3002244 ,  0.98755945, -0.04839043, -0.5464203 ]]), array([[-0.06577027, -0.00649488,  0.09945671,  0.05017592,  0.01931199,\n",
      "         0.06615149,  0.26703279,  0.00595788,  0.08656772,  0.37335493]]), array([[ 0.05326346,  0.95650411, -0.07625684,  0.01146337,  0.02037133,\n",
      "        -0.05608215, -0.24757512,  0.0039806 , -0.00421827,  0.13655158]])]\n",
      "true values : \n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#from network import Network\n",
    "#from fc_layer import FCLayer\n",
    "#from activation_layer import ActivationLayer\n",
    "#from activations import tanh, tanh_prime\n",
    "#from losses import mse, mse_prime\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "x_train = standardize(x_train)\n",
    "\n",
    "x_test = standardize(x_test)\n",
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train on 1000 samples\n",
    "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(x_test[0:3])\n",
    "print(\"\\n\")\n",
    "print(\"predicted values : \")\n",
    "print(out, end=\"\\n\")\n",
    "print(\"true values : \")\n",
    "print(y_test[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjmbNPVdKseU",
    "outputId": "5021b190-5a3d-4e30-82af-871f7c7671d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       980\n",
      "           1       0.93      0.93      0.93      1135\n",
      "           2       0.75      0.70      0.72      1032\n",
      "           3       0.82      0.66      0.73      1010\n",
      "           4       0.77      0.60      0.67       982\n",
      "           5       0.73      0.53      0.61       892\n",
      "           6       0.70      0.76      0.73       958\n",
      "           7       0.79      0.80      0.80      1028\n",
      "           8       0.68      0.59      0.64       974\n",
      "           9       0.44      0.76      0.56      1009\n",
      "\n",
      "    accuracy                           0.72     10000\n",
      "   macro avg       0.74      0.72      0.72     10000\n",
      "weighted avg       0.75      0.72      0.73     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [np.argmax(net.predict(x)) for x in x_test]\n",
    "y_true = [np.argmax(y) for y in y_test]\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "#Standardizing"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
